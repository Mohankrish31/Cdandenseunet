import os
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import lpips
from math import exp
import sys

# ====================================================================
# CDANDenseUNet Model Definition
# ====================================================================

# ====================== CBAM ======================
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, max(1, in_channels // reduction), 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(max(1, in_channels // reduction), in_channels, 1, bias=False),
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        attn = self.sigmoid(avg_out + max_out)
        return x * attn

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        padding = (kernel_size - 1) // 2
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        cat = torch.cat([avg_out, max_out], dim=1)
        attn = self.sigmoid(self.conv(cat))
        return x * attn

class CBAM(nn.Module):
    def __init__(self, channels, reduction=16, kernel_size=7):
        super().__init__()
        self.ca = ChannelAttention(channels, reduction)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        x = self.ca(x)
        x = self.sa(x)
        return x

# ================= Dense blocks (fixed) =================
class _DenseLayer(nn.Module):
    def __init__(self, in_channels, growth_rate, drop_rate=0.0):
        super().__init__()
        self.norm = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)
        self.drop_rate = drop_rate

    def forward(self, x):
        out = self.conv(self.relu(self.norm(x)))
        if self.drop_rate > 0:
            out = F.dropout(out, p=self.drop_rate, training=self.training)
        return torch.cat([x, out], dim=1)

class _DenseBlock(nn.Module):
    def __init__(self, num_layers, in_channels, growth_rate, drop_rate=0.0):
        super().__init__()
        layers = []
        channels = in_channels
        for _ in range(num_layers):
            layers.append(_DenseLayer(channels, growth_rate, drop_rate))
            channels += growth_rate
        self.block = nn.Sequential(*layers)
        self.out_channels = channels

    def forward(self, x):
        return self.block(x)

class Bottleneck(nn.Module):
    def __init__(self, in_channels, num_layers, growth_rate, drop_rate=0.0):
        super().__init__()
        self.dense_block = _DenseBlock(num_layers, in_channels, growth_rate, drop_rate)
        self.out_channels = self.dense_block.out_channels

    def forward(self, x):
        return self.dense_block(x)

# ============ Encoder & Decoder (with consistent shapes) ============
class EncoderBlock(nn.Module):
    def __init__(self, in_channels, growth_rate, num_layers=2, use_cbam=False):
        super().__init__()
        self.db = _DenseBlock(num_layers=num_layers, in_channels=in_channels, growth_rate=growth_rate)
        self.cbam = CBAM(self.db.out_channels) if use_cbam else None
        self.pool = nn.MaxPool2d(2)

    @property
    def out_channels(self):
        return self.db.out_channels

    def forward(self, x):
        x = self.db(x)
        if self.cbam:
            x = self.cbam(x)
        skip = x
        x = self.pool(x)
        return x, skip

class DecoderBlock(nn.Module):
    def __init__(self, in_channels, skip_channels, num_layers=2, use_cbam=False):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_channels, skip_channels, kernel_size=2, stride=2)
        self.db = _DenseBlock(num_layers=num_layers,
                              in_channels=2 * skip_channels,
                              growth_rate=skip_channels // 2)
        self.compress = nn.Conv2d(2 * skip_channels + num_layers * (skip_channels // 2),
                                  skip_channels, kernel_size=1, bias=False)
        self.cbam = CBAM(skip_channels) if use_cbam else None

    @property
    def out_channels(self):
        return self.compress.out_channels if hasattr(self.compress, 'out_channels') else None

    def forward(self, x, skip):
        x = self.up(x)
        if x.shape[-1] != skip.shape[-1] or x.shape[-2] != skip.shape[-2]:
            dh = skip.shape[-2] - x.shape[-2]
            dw = skip.shape[-1] - x.shape[-1]
            skip = skip[:, :, dh // 2: skip.shape[-2] - (dh - dh // 2),
                              dw // 2: skip.shape[-1] - (dw - dw // 2)]
        x = torch.cat([x, skip], dim=1)
        x = self.db(x)
        x = self.compress(x)
        if self.cbam:
            x = self.cbam(x)
        return x

class CDANDenseUNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, base_channels=24, growth_rate=12):
        super().__init__()
        self.init_conv = nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.enc1 = EncoderBlock(in_channels=base_channels, growth_rate=growth_rate, num_layers=2, use_cbam=False)
        self.enc2 = EncoderBlock(in_channels=self.enc1.out_channels, growth_rate=growth_rate, num_layers=2, use_cbam=False)
        self.enc3 = EncoderBlock(in_channels=self.enc2.out_channels, growth_rate=growth_rate, num_layers=2, use_cbam=True)
        self.bottleneck = Bottleneck(in_channels=self.enc3.out_channels, num_layers=2, growth_rate=growth_rate)
        self.cbam_bottleneck = CBAM(self.bottleneck.out_channels)
        self.dec3 = DecoderBlock(in_channels=self.bottleneck.out_channels, skip_channels=self.enc3.out_channels, num_layers=2, use_cbam=False)
        self.dec2 = DecoderBlock(in_channels=self.enc3.out_channels, skip_channels=self.enc2.out_channels, num_layers=2, use_cbam=False)
        self.dec1 = DecoderBlock(in_channels=self.enc2.out_channels, skip_channels=self.enc1.out_channels, num_layers=2, use_cbam=True)
        self.final = nn.Conv2d(self.enc1.out_channels, out_channels, kernel_size=1)
        self.final_activation = nn.Sigmoid()

    def forward(self, x):
        x = self.init_conv(x)
        x, s1 = self.enc1(x)
        x, s2 = self.enc2(x)
        x, s3 = self.enc3(x)
        x = self.bottleneck(x)
        x = self.cbam_bottleneck(x)
        x = self.dec3(x, s3)
        x = self.dec2(x, s2)
        x = self.dec1(x, s1)
        out = self.final(x)
        out = self.final_activation(out)
        return out

# ====================================================================
# Training and Validation Script
# ====================================================================

# Reproducibility (optional)
def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
set_seed(42)

# Paths (edit if needed)
train_low_dir  = "/content/cvccolondbsplit/train/low"
train_high_dir = "/content/cvccolondbsplit/train/high"
val_low_dir    = "/content/cvccolondbsplit/val/low"
val_high_dir   = "/content/cvccolondbsplit/val/high"
save_dir       = "/content/saved_model"
os.makedirs(save_dir, exist_ok=True)

# Hyperparameters
learning_rate = 5e-4
weight_decay  = 1e-5
num_epochs    = 100
batch_size    = 8
early_stopping_patience = 10
grad_clip_norm = 1.0

# Dataset
class PairedImageFolder(Dataset):
    def __init__(self, low_dir, high_dir, transform=None):
        self.low_dir = low_dir
        self.high_dir = high_dir
        self.transform = transform
        self.names = sorted([n for n in os.listdir(low_dir)
                             if n.lower().endswith((".png", ".jpg", ".jpeg"))])
    def __len__(self):
        return len(self.names)
    def __getitem__(self, idx):
        name = self.names[idx]
        low_img  = Image.open(os.path.join(self.low_dir, name)).convert("RGB")
        high_img = Image.open(os.path.join(self.high_dir, name)).convert("RGB")
        if self.transform:
            low_img  = self.transform(low_img)
            high_img = self.transform(high_img)
        return low_img, high_img, name

# Losses
class SobelEdgeLoss(nn.Module):
    def __init__(self):
        super().__init__()
        sobel_x = torch.tensor([[[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]]], dtype=torch.float32)
        sobel_y = torch.tensor([[[-1., -2., -1.], [0., 0., 0.], [1., 2., 1.]]], dtype=torch.float32)
        self.register_buffer("sobel_x", sobel_x.unsqueeze(0))
        self.register_buffer("sobel_y", sobel_y.unsqueeze(0))
    def forward(self, pred, target):
        pred_gray   = pred.mean(dim=1, keepdim=True)
        target_gray = target.mean(dim=1, keepdim=True)
        pred_gx   = F.conv2d(pred_gray, self.sobel_x, padding=1)
        pred_gy   = F.conv2d(pred_gray, self.sobel_y, padding=1)
        target_gx = F.conv2d(target_gray, self.sobel_x, padding=1)
        target_gy = F.conv2d(target_gray, self.sobel_y, padding=1)
        pred_grad   = torch.sqrt(pred_gx**2 + pred_gy**2 + 1e-6)
        target_grad = torch.sqrt(target_gx**2 + target_gy**2 + 1e-6)
        return F.l1_loss(pred_grad, target_grad)

def gaussian(window_size, sigma):
    g = torch.tensor([exp(-(x - window_size//2)**2 / float(2*sigma**2)) for x in range(window_size)], dtype=torch.float32)
    return g / g.sum()

def create_window(window_size, channel):
    _1D = gaussian(window_size, 1.5).unsqueeze(1)
    _2D = (_1D @ _1D.t()).float().unsqueeze(0).unsqueeze(0)
    return _2D.expand(channel, 1, window_size, window_size).contiguous()

class SSIMLoss(nn.Module):
    def __init__(self, window_size=11):
        super().__init__()
        self.window_size = window_size
        self.channel = 1
        self.register_buffer("window", create_window(window_size, self.channel))
    def forward(self, img1, img2):
        (_, ch, _, _) = img1.size()
        if ch != self.channel or self.window.size(0) != ch:
            self.window = create_window(self.window_size, ch).to(img1.device)
            self.channel = ch
        mu1 = F.conv2d(img1, self.window, padding=self.window_size//2, groups=ch)
        mu2 = F.conv2d(img2, self.window, padding=self.window_size//2, groups=ch)
        mu1_sq, mu2_sq, mu1_mu2 = mu1**2, mu2**2, mu1*mu2
        sigma1_sq = F.conv2d(img1*img1, self.window, padding=self.window_size//2, groups=ch) - mu1_sq
        sigma2_sq = F.conv2d(img2*img2, self.window, padding=self.window_size//2, groups=ch) - mu2_sq
        sigma12   = F.conv2d(img1*img2, self.window, padding=self.window_size//2, groups=ch) - mu1_mu2
        C1, C2 = 0.01**2, 0.03**2
        ssim_map = ((2*mu1_mu2 + C1) * (2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        return 1 - ssim_map.mean()

mse_loss_fn  = nn.MSELoss()
edge_loss_fn = SobelEdgeLoss()
ssim_loss_fn = SSIMLoss()

def total_loss_fn(pred, target, w_mse, w_lpips, w_edge, w_ssim, lpips_model):
    # LPIPS expects [-1,1]
    lp = lpips_model(2*pred - 1, 2*target - 1).mean()
    mse  = mse_loss_fn(pred, target)
    edge = edge_loss_fn(pred, target)
    ssim = ssim_loss_fn(pred, target)
    total = w_mse*mse + w_lpips*lp + w_edge*edge + w_ssim*ssim
    return total, {"mse": mse.item(), "lpips": lp.item(), "edge": edge.item(),
                   "ssim": ssim.item()}

# Device, LPIPS, Transforms, Loaders
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])
train_set = PairedImageFolder(train_low_dir, train_high_dir, transform)
val_set   = PairedImageFolder(val_low_dir, val_high_dir, transform)
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

# Model & Optimizer
model = CDANDenseUNet(in_channels=3, base_channels=32).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

# loss weights (new, balanced weights)
w_mse, w_lpips, w_edge, w_ssim = 0.20, 0.40, 0.30, 0.10

# Training Loop
best_val_loss = float("inf")
patience = 0
train_losses, val_losses = [], []
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

for epoch in range(1, num_epochs + 1):
    # ---- TRAIN ----
    model.train()
    running = 0.0
    for idx, (low, high, _) in enumerate(train_loader):
        low, high = low.to(device), high.to(device)
        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
            pred = model(low)
            loss, _ = total_loss_fn(pred, high, w_mse, w_lpips, w_edge, w_ssim, lpips_loss_fn)
        
        scaler.scale(loss).backward()
        
        # Gradient Monitoring
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1./2)
        print(f"Epoch {epoch} | Batch {idx+1} (Train) | Loss: {loss.item():.4f} | Grad Norm: {total_norm:.6f}")

        if grad_clip_norm is not None:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
        scaler.step(optimizer)
        scaler.update()
        running += loss.item()
    avg_train = running / max(len(train_loader), 1)
    train_losses.append(avg_train)

    # ---- VALIDATE ----
    model.eval()
    val_running = 0.0
    with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
        for idx, (low, high, _) in enumerate(val_loader):
            low, high = low.to(device), high.to(device)
            pred = model(low)
            vloss, _ = total_loss_fn(pred, high, w_mse, w_lpips, w_edge, w_ssim, lpips_loss_fn)
            val_running += vloss.item()
    avg_val = val_running / max(len(val_loader), 1)
    val_losses.append(avg_val)

    print(f"Epoch {epoch:03d} | Train {avg_train:.4f} | Val {avg_val:.4f}")
    
    # ---- SAVE BEST ----
    if avg_val < best_val_loss:
        best_val_loss = avg_val
        patience = 0
        torch.save(model, os.path.join(save_dir, "cdandenseunet.pt"))
        torch.save(model.state_dict(), os.path.join(save_dir, "cdan_denseunet_weights.pth"))
        print("  ✔ Saved: cdandenseunet.pt and cdan_denseunet_weights.pth")
    else:
        patience += 1
        if patience >= early_stopping_patience:
            print(f"⏹ Early stopping at epoch {epoch}")
            break

print("Training complete. Best model saved in:", save_dir)
