import os
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import lpips
from math import exp

# ---- your model ----
from models.cdan_denseunet import CDANDenseUNet

# -----------------------------
# Reproducibility (optional)
# -----------------------------
def set_seed(seed=42):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
set_seed(42)

# -----------------------------
# Paths (edit if needed)
# -----------------------------
train_low_dir  = "/content/cvccolondbsplit/train/low"
train_high_dir = "/content/cvccolondbsplit/train/high"
val_low_dir    = "/content/cvccolondbsplit/val/low"
val_high_dir   = "/content/cvccolondbsplit/val/high"
save_dir       = "/content/saved_model"
os.makedirs(save_dir, exist_ok=True)

# -----------------------------
# Hyperparameters
# -----------------------------
learning_rate = 1e-4
weight_decay  = 1e-5
num_epochs    = 100
batch_size    = 8
early_stopping_patience = 10
grad_clip_norm = 1.0  # set None to disable

# -----------------------------
# Dataset
# -----------------------------
class PairedImageFolder(Dataset):
    """
    Expects identical filenames in low/ and high/ folders.
    """
    def __init__(self, low_dir, high_dir, transform=None):
        self.low_dir = low_dir
        self.high_dir = high_dir
        self.transform = transform
        self.names = sorted([n for n in os.listdir(low_dir)
                             if n.lower().endswith((".png", ".jpg", ".jpeg"))])
    def __len__(self):
        return len(self.names)
    def __getitem__(self, idx):
        name = self.names[idx]
        low_img  = Image.open(os.path.join(self.low_dir,  name)).convert("RGB")
        high_img = Image.open(os.path.join(self.high_dir, name)).convert("RGB")
        if self.transform:
            low_img  = self.transform(low_img)
            high_img = self.transform(high_img)
        return low_img, high_img, name

# -----------------------------
# Losses
# -----------------------------
class SobelEdgeLoss(nn.Module):
    def __init__(self):
        super().__init__()
        sobel_x = torch.tensor([[[-1., 0., 1.],
                                 [-2., 0., 2.],
                                 [-1., 0., 1.]]], dtype=torch.float32)
        sobel_y = torch.tensor([[[-1., -2., -1.],
                                 [ 0.,  0.,  0.],
                                 [ 1.,  2.,  1.]]], dtype=torch.float32)
        self.register_buffer("sobel_x", sobel_x.unsqueeze(0))
        self.register_buffer("sobel_y", sobel_y.unsqueeze(0))
    def forward(self, pred, target):
        pred_gray   = pred.mean(dim=1, keepdim=True)
        target_gray = target.mean(dim=1, keepdim=True)
        pred_gx   = F.conv2d(pred_gray,   self.sobel_x, padding=1)
        pred_gy   = F.conv2d(pred_gray,   self.sobel_y, padding=1)
        target_gx = F.conv2d(target_gray, self.sobel_x, padding=1)
        target_gy = F.conv2d(target_gray, self.sobel_y, padding=1)
        pred_grad   = torch.sqrt(pred_gx**2 + pred_gy**2 + 1e-6)
        target_grad = torch.sqrt(target_gx**2 + target_gy**2 + 1e-6)
        return F.l1_loss(pred_grad, target_grad)

def gaussian(window_size, sigma):
    g = torch.tensor([exp(-(x - window_size//2)**2 / float(2*sigma**2))
                      for x in range(window_size)], dtype=torch.float32)
    return g / g.sum()

def create_window(window_size, channel):
    _1D = gaussian(window_size, 1.5).unsqueeze(1)
    _2D = (_1D @ _1D.t()).float().unsqueeze(0).unsqueeze(0)
    return _2D.expand(channel, 1, window_size, window_size).contiguous()

class SSIMLoss(nn.Module):
    def __init__(self, window_size=11):
        super().__init__()
        self.window_size = window_size
        self.channel = 1
        self.register_buffer("window", create_window(window_size, self.channel))
    def forward(self, img1, img2):
        (_, ch, _, _) = img1.size()
        if ch != self.channel or self.window.size(0) != ch:
            self.window = create_window(self.window_size, ch).to(img1.device)
            self.channel = ch
        mu1 = F.conv2d(img1, self.window, padding=self.window_size//2, groups=ch)
        mu2 = F.conv2d(img2, self.window, padding=self.window_size//2, groups=ch)
        mu1_sq, mu2_sq, mu1_mu2 = mu1**2, mu2**2, mu1*mu2
        sigma1_sq = F.conv2d(img1*img1, self.window, padding=self.window_size//2, groups=ch) - mu1_sq
        sigma2_sq = F.conv2d(img2*img2, self.window, padding=self.window_size//2, groups=ch) - mu2_sq
        sigma12   = F.conv2d(img1*img2, self.window, padding=self.window_size//2, groups=ch) - mu1_mu2
        C1, C2 = 0.01**2, 0.03**2
        ssim_map = ((2*mu1_mu2 + C1) * (2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
        return 1 - ssim_map.mean()

# REMOVED RangeLoss

mse_loss_fn  = nn.MSELoss()
edge_loss_fn = SobelEdgeLoss()
ssim_loss_fn = SSIMLoss()

def total_loss_fn(pred, target, w_mse, w_lpips, w_edge, w_ssim, lpips_model):
    # LPIPS expects [-1,1]
    lp = lpips_model(2*pred - 1, 2*target - 1).mean()
    mse  = mse_loss_fn(pred, target)
    edge = edge_loss_fn(pred, target)
    ssim = ssim_loss_fn(pred, target)
    total = w_mse*mse + w_lpips*lp + w_edge*edge + w_ssim*ssim
    return total, {"mse": mse.item(), "lpips": lp.item(), "edge": edge.item(),
                   "ssim": ssim.item()}

# -----------------------------
# Device, LPIPS, Transforms, Loaders
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])
train_set = PairedImageFolder(train_low_dir, train_high_dir, transform)
val_set   = PairedImageFolder(val_low_dir,   val_high_dir,   transform)
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

# -----------------------------
# Model & Optimizer
# -----------------------------
model = CDANDenseUNet(in_channels=3, base_channels=32).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

# loss weights (adjusting to re-balance after removing RangeLoss)
w_mse, w_lpips, w_edge, w_ssim = 0.45, 0.10, 0.20, 0.25

# -----------------------------
# Training Loop
# -----------------------------
best_val_loss = float("inf")
patience = 0
train_losses, val_losses = [], []
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

for epoch in range(1, num_epochs + 1):
    # ---- TRAIN ----
    model.train()
    running = 0.0
    for low, high, _ in train_loader:
        low, high = low.to(device), high.to(device)
        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
            pred = model(low)
            # üí° Sanity check for output values
            # This is key to diagnosing the black box issue
            if pred.max().item() < 0.1:
                print(f"Epoch {epoch} | Warning: Max output value is very low: {pred.max().item():.4f}")
            loss, _ = total_loss_fn(pred, high, w_mse, w_lpips, w_edge, w_ssim, lpips_loss_fn)
        scaler.scale(loss).backward()
        if grad_clip_norm is not None:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
        scaler.step(optimizer)
        scaler.update()
        running += loss.item()
    avg_train = running / max(len(train_loader), 1)
    train_losses.append(avg_train)

    # ---- VALIDATE ----
    model.eval()
    val_running = 0.0
    with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
        for low, high, _ in val_loader:
            low, high = low.to(device), high.to(device)
            pred = model(low)
            vloss, _ = total_loss_fn(pred, high, w_mse, w_lpips, w_edge, w_ssim, lpips_loss_fn)
            val_running += vloss.item()
    avg_val = val_running / max(len(val_loader), 1)
    val_losses.append(avg_val)
    print(f"Epoch {epoch:03d} | Train {avg_train:.4f} | Val {avg_val:.4f}")
    # ---- SAVE BEST ----
    if avg_val < best_val_loss:
        best_val_loss = avg_val
        patience = 0
        # save full model and weights
        torch.save(model, os.path.join(save_dir, "cdandenseunet.pt"))
        torch.save(model.state_dict(), os.path.join(save_dir, "cdan_denseunet_weights.pth"))
        print("  ‚úî Saved: cdandenseunet.pt and cdan_denseunet_weights.pth")
    else:
        patience += 1
        if patience >= early_stopping_patience:
            print(f"‚èπ Early stopping at epoch {epoch}")
            break

print("Training complete. Best model saved in:", save_dir)
